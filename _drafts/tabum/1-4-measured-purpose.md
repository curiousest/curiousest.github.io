"So you're an enlightenment douche?"

"No. I mean maybe, but that's not what's wrong."

"Well, let me play it back to you, what you just said. First, you've been having these big changes in your life’s purpose. So... repeated enlightenment, yeah?"

"Yup."

"And you tell, like, *everyone*. So they can see what you see - those new beliefs."

"Yeah, to share it. It’s so nice when everyone feels the same way."

"Yes, you’re a superspreader... Second - you said that your purpose is broken? So the service we provide to every human… is broken? That calculation made of all public sensor data about you ever captured, approximating all the belief structures deep in your brain, estimating every past purpose you’ve ever had. It’s broken. And you think your purpose has been broken for... months?"

"Umm... Yeah, I, uh. Can't escape this purpose. But that’s not the problem."

"Dude... I'm sorry this is happening to you. It must be pretty overwhelming. If that was happening to me, yeah I'd be messed up too."

Dan paused for a moment to absorb the information and empathize with Alex. Thinking about what mentorship to provide. Dan's eyes drifted around the transparent-open office. Just about everything was see-through. Everything in the building except humans, so Dan could see all the people, on all floors above and below, float-walking about their day. A colleague standing in the kitchen six meters away caught his eye. Stephan - a Deacon of SRE, on his third coffee of the morning. Stephan was looking rough, like caffeine was today's crutch. Dan glanced at Stephan's purpose.

```
Purpose of Stephan (from Dan's perspective):

past minute: "Consume caffeine." (0.9 confidence)
past hour: "Debug faulty system." (0.8)
past day: "Party." (0.4)
past week: 
- "Show off hard work to boss." (0.3)
- "Hook up on nights out." (0.2)
past month: "Get promotion." (0.5)
past year: "Get higher social status." (0.6)
lifetime: "Individual survive." (0.8)
```

Stephan glanced at a notification, saw that Dan had observed his purpose, rendered from Dan's perspective. Stephan looked over through walls at Dan and Alex huddled in a booth, gestured devils horns with right-hand index and pinky fingers pointed up and stuck his tongue out. Dan started to chuckle, remembered he was in a serious meeting, then disguised his laugh with a cough. He switched his AR (augmented reality) eyeset to "one-on-one" mode and Stephan disappeared. Along with all the people in Poise, he was able to see through the transparent walls and ceilings and floors and doors - all of the people disappeared, made transparent too. All he could see was the ground a few stories down, the horizon, the cloud-numbed sky, and Alex float-sitting two meters in front of him.

Dan had long moved on from DoL. Buildings move too slowly so he dove into the realm of dolphin logos. The exponential growth scene, where viral living orgs multiply. He was ordained in Poise, a Series-B epidemic organizationism that calculates the historical purpose of every single thing known to humankind.
> The “Poise” distributed purpose calculator used the endless public sensor data streaming from every inch of the world. Poise estimated the beliefs embedded in any thing using the observed decisions that thing made, as well as decisions that resulted from that thing’s effect on the world. Poise could derive the emergent optimization objectives (aka purpose) that thing follows from having those estimated beliefs. In effect, Poise estimated the beliefs and therefore purpose any given "thing" - everything from a pomegranate to a house or even a human. Purpose is relative, and all Poise calculations are made relative to the viewer of the purpose, as if the viewer had consumed all the public sensor data about the subject to come to their own conclusions about its purpose.

Dan glanced at his own last five minutes of purpose. It read, 
```
Self-purpose, past five minutes: “provide mentorship to improve the mentee".
```
That's what he was going for. He nodded in self-approval. All this time, Alex had been fidgeting on the other side of the table. Alex finally had enough of the silence, continued,

"That's not my real problem though. That's like, whatever, it's been like that for a long time and I'm over it. The problem is that I'm trying to get these Order of CV data scientists - uh, you know, the Order of Cross-Validation? They do model evaluation. I want them to build a model for this theory I have, but they're not listening."

"Oh yeah? Why not?”
Dan said. Dan was now a bishop-level engineer in Poise. He oversaw the parts of the system where data scientists apply their research. Where they build or improve all kinds of Poise models. Models to improve the performance of existing purpose calculations. Models to identify more detailed purposes. When a new theory was published in peer review, someone would apply it to Poise by building and integrating a model to see if the theory had higher predictive power than other models. If the model was a net improvement, the new model was gradually incorporated into the distributed Poise system.

Alex answered,
"I dunno. Some politics crap. Something like... it doesn't align with their goals this cycle. And the Bishop of Cross-Validation hates me. Seriously, he hates me. And this model I want to build is obviously better than whatever they're doing right now. I'm tired of this. I'm so bad at this 4D chess politics stuff."

Dan was confident that the Bishop of Cross-Validation was being more reasonable than Alex. Dan considered how to diagnose Alex's problem. Maybe Alex didn't understand what the Order of CV worked on? Dan asked,
"Can we compare purposes for a moment - what has the Order of CV been doing for the past few months?"

"Sure.”
said Alex. Dan brought up the purpose and beliefs of the Order of CV for this quarter from his perspective. Alex brought up the purpose from his perspective to compare side-by-side.

```
Purpose of the Order of CV (Dan's perspective):

past month: reduce distributed model evaluation run time by 80%. (0.7)
past quarter: reduce model development feedback loop by 90%. (0.6)
```

```
Purpose of the Order of CV (Alex's perspective):

past month: model evaluation run time takes one hour. (0.7)
past quarter: model development feedback loop takes one day. (0.6)
```

Dan said,
"We're not far off in what we're seeing from them. So how do you think your proposal fits into their goals?"

Alex squirmed in his chair and stared off. He knew he was still right, but wasn't able to communicate how. Dan could tell that whatever Alex had in mind was not in line with the goals of the Order of CV.

Dan said,
"Ok, how about instead, you tell me more about your own purpose? I'm getting the feeling there's something happening in there. Maybe I can help."

Alex answered,
"Ok, but I don't see how they could be related. Last time I checked, the belief calculator said something strange about enlightenment. I'd need a specialized priest of data analytics to make sense of it."

Dan looked deeper at Alex's purposes, trying to understand Alex's perspective on Alex's own purpose.

```
Purpose of Alex (Alex's perspective):

past hour: "Convince Order of CV to build ‘Ground Truth’." (0.9)
past day: "Make ‘Ground Truth’ happen." (0.8)
past week: "Make ’Ground Truth’ happen." (0.6)
past month: "Chase the feeling of enlightenment." (0.5)
past year: "Chase enlightenment." (0.5)
lifetime: "Survive - individual human." (0.6)
```

Alex got a ping, notifying him that Dan had looked at his purpose and beliefs. Some people cared about when others viewed their purpose, and Alex was one of them. Looking at someone's purpose formulae and beliefs was like looking at someone - look but don't stare. 
> People can control only so much of their appearance, just like they can control only so much of their purpose and beliefs. It was rude in this culture to stare at someone's purpose and beliefs without their consent - to overanalyze the information. But anyone was free to look at anyone else's purpose or beliefs at any time, since it was infeasible to hide that information. Just like anyone could look at anyone else in a shared space, or look at their publicly available profile data. And when a purpose was viewed, everyone could see exactly how much effort a purpose-peeker was putting into processing another person’s purpose. Further, unsolicited attempts to change another's beliefs were socially unacceptable if not illegal. Most people avoided thinking critically about their purpose, especially without an existentialism-dampening aid.

Dan had processed Alex's purposes and beliefs. Dan said,
“Nice, yeah you're right those purposes are pretty weird. I’ve never seen the enlightenment ones before. But anyways. What's this ‘Ground Truth’ project you’re working on?”

Alex's eyes lit up, widened, then went up-right bringing his head tilting up with them. His mouth was slightly open, half-mouthing abstractions and hands dancing around his ideas-in-mind. First loading large chunks of data into memory and reshaping the abstractions into a coherent whole. Alex had gotten there now, next step was to find an entrypoint to explain. A mixed top-down, bottom-up explanation. Dan could see him processing it all, knew he was going straight into the details, knew Alex was a few levels below anything that would make sense, so this was going to take a lot of active listening. Dan belted himself in for a monologue.

“I want to automate trust.”
Alex started, watching for a reaction in Dan's face. Dan raised his eyebrows. Alex felt a burst of validation and started again,
> “So Poise is powered by all this public sensor data owned by a mix of everyone in the public. We’re the first org to put all of it together and try to make sense of it. Then we apply a bunch of models to derive beliefs and purposes of things. So we're running two massively distributed tasks: data collection and belief-purpose processing. But we're so focused on purpose - what if we did something else with that data?

Alex paused for effect. To let Dan wonder a moment. He started again with excessive excitement,
> Enter ‘Ground Truth’. A public ledger to determine the reality about every thing in the world. Data sources will all contribute to the public ledger of the world and a distributed processing system would agree on the ground truth of the things those sensors are observing.”

Alex stopped for a breath, continued,
> “So like, take this conversation, for example. Let's say one of the ‘things’ here that can have a truth is the content of our conversation. There's like, at least five microphones and three cameras picking up our conversation. All of them could write data to the public ledger, and a separate distributed system could consume all those different data sources, and algorithmically agree on the reality of ‘our conversation’ - who was there and what was said. So there would be a public record of our presence and our conversation.”

Dan thought for a moment.
“So we rely on data coming from networks of devices. But surely someone could hack those devices so that they send a different version of reality to the public ledger than the one we had. Someone could hack them to *lie*.”

“Oh, absolutely. But see, public ledgers can make it too difficult to tell a *convincing lie*.”
Alex started.
> “There are so many minute details that could uncover the lie. The more data published to the public ledger, the exponentially harder it becomes to tell a convincing lie. So for our conversation, let's say we hacked all the microphones and cameras in the room. There's still endless sensors both in and out of the room. The lie conversation would have to be exactly the same length of time, otherwise sensors in the hallway outside of this room would disagree with the lie. And any audible sounds outside of the room would have to be accounted for in the lie. And we're both wearing health monitors - that data would have to be aligned with our speaking patterns and the flow of conversation. And you checked my purpose and The Order of CV’s purpose during this conversation, so those lookups would have to be integrated into the lie at exactly the right points in time too. And that's just off the top of my head. I'm sure with all the data we're collecting, there's tons more ways to catch a lie.”

Dan continued playing a weak devil's advocate,
“Hmmm. What about false positives for lies. Like accidentally false - any of those sensors could be a little off on their own. How do you tell the difference between random noise and a lie?”

“Yeah totally possible. But noise can be modelled and the inevitable improvements to technology will uncover historical lies.”
Alex answered,
> “But noise and lies diverge exponentially over time. As the number of independently-controlled sensors goes up, and the number of independent nodes that process public ledger data goes up, it gets exponentially more expensive to get the majority of sensors or processors to contribute to a consistent lie. Because you have to gain control over more than half of the independent actors. And at the end of the day, the universe has infinite sensors, and we only know about some of them. Liars can’t protect themselves against the future. Technological advancement inevitably uncovers lies. Information wants to be free. Truth is inevitable.

Dan thought, then said,
“But you'd eliminate privacy.”

“Yes, exactly. Privacy must be the price of power.”
Alex answered,
> “What is your privacy for? It's to stop people and systems from using your data to benefit them and harm you. But the people taking advantage of you would be doing it in plain sight. Everyone would know. When society observes that behaviour, we'd stamp it out at the source.

”Oh really? It’s that simple, eh?”
Dan said.

“It’s a starting point, at least.”
Alex continued,
> “We'd figure out how to deal with it. See the fundamental problem is that trust is expensive. I figure humankind spends more than half the world's GDP on trust - government, military, banks, law, and ‘management’ alone are probably that much. All that spend is for establishing the trust that the people at the top are doing things in the best interests of all people. And if we automate trust, we will reduce so much of that waste. We can focus on happiness and scientific advancement. Trust is one of life’s most expensive resources.”

“Perfect is the enemy of better.”
Dan pointed out.
> “So you're proposing a perfect information utopia. The problem with idealistic thoughts is that you're often more right about a lot of important things, but you're always extremely wrong about something that ruins everything. That blind devotion you’re riding makes you feel so fulfilled that you ignore all the consequences. You can be right about it being a great space to make improvements, but wrong about the destination and path to get there. In the real world, there are endless things to improve, but we have limited effort to put in. And so we prioritize what to work on - that means dropping most ideas and getting a fraction of the budget for the ones we do work on. What you're describing is too big a disruption to achieve any time soon. And you trust some future overlords not to use this system to seize control? And what if we need privacy for other reasons beyond protecting ourselves from being taken advantage of?”

“Right, but we shouldn't trust the people and organizations at the top who have power. That’s exactly the trust we should focus on automating. The common people at the bottom should keep their privacy.”
Alex answered.
> “The problem there is we can never trust the people at the top to make optimal decisions for humankind and we don't trust the general public to behave either. We just assume they'd destroy all diversity of thought - but that's obviously suboptimal. There's a way to fix those problems, and it involves more automation of trust, not less. More transparency, not more privacy - for those with power. That’s the overall direction humankind is going anyways. Are we collecting and sharing less data over time? No, of course not. We collect and share an order of magnitude more data every generation. Do you really think we're ever going to reverse course? Of course not - technology marches forwards and information wants to be free. That is optimal. What I'm saying isn't just ‘the optimal thing to do’ - it's inevitable. If we don't start automating trust now, someone else will do it.”

Dan was getting impatient. This was clearly high-level idealistic nonsense, and now he wanted to hear something tangible.
“It's quite the ideology. Poise sells trust, so automating trust isn’t exactly in our best interests. And what does this have to do with The Order of CV?”

“I want them to make a model that flags misbehaving data sources. A model that says when the purpose of a data source is to lie.”
Alex answered.

Dan squinted for a second,
“That actually sounds pretty reasonable. What did they say?”

“That it's not possible. They said nothing can have the purpose, 'to lie'.”
Alex answered.

Dan said,
“Interesting. Truth does boil down to the sensors you use to ground it - the assumptions used to make those sensors. You know, we can't even trust our eyes around here. The building assumes we don't want to see certain people, so we don't. There's building staff all around - cleaners, security, maintenance workers - but pretty much no-one at Poise knows because they're hidden in the default AR mode.”
Dan paused to think a bit more, then said,
“Let me talk to some people and get back to you on this. Ok?”

“Sure.”
Alex said.

---

The next week, Dan asked around.

Dan had a coffee with the Cardinal of Model Delivery. She complained about noise from misbehaving sensors. The inconsistencies reduced the accuracy of purpose models across the system.

Dan had a one-on-one with the Archbishop of Core Infra. He detailed the recurring tech debt from the lack of standardization. Everyone was DIY-ing their own data cleaning workloads and expecting him to maintain a mess of historical infra decisions. Then, instead of fixing their junk, they wanted to run some ML workloads that predicted when their junk was going to break and smooth it over. All for some hacked-together deduplication and time-series data alignment implementations. And they want him to maintain it.

Dan had lunch with the Archbishop of Labelling Ops. She had been labouring over finding the right datasets to label - picking the ones that most consistently represented the data. She felt like she had too much influence over the purpose models and she was unfairly influencing the datasets. It was far too hard for her decisions to be unbiased. She explained how she had just about given up being unbiased because every week she found a new bias to correct for, and it was becoming impossible to keep relabelling every collection, for every correction.

Dan chatted with the Cardinal of CV before the Pope-Poise's weekly laying of all-hands. The Cardinal complained how his team kept having to rerun workloads and redo work because of upstream problems in source data and labelling. He wanted to do some innovative things to predictively reschedule workloads, but he was getting pushback from the Archbishop of Core Infra.

---

The next week, Dan and Alex met in the office to start their one-on-one. No room booked. They scanned the first hall, window shopping for a meeting room. A Cardinal was sitting alone in a meeting room, concentrating. Too senior, can't bother him. Ten people were packed into a four-person room, standing and sitting on the floor. The next room had two people who glared back at Dan and Alex as they scanned past, clearly discussing a sensitive matter. The next room had one of Poise's priest-level recruiters evangelising a stranger.

No rooms. Org growing pains. They looked towards the board-chancel room. Filled with a radiance of Cardinals hosting important visitors. They checked out the kitchen to see if there was space there. All the booths were taken for lunchtime. They walked the open-chapel office, but there were no quiet spaces. There was far too much going on for anyone to be getting any work done.

Dan said,
"How about we go out for lunch instead?"

"Yeah sure."
Alex answered.

They headed to the exit. While they were walking, Dan said,
"By the way, about that project you mentioned to me last week - everyone across the org seems to be seeing some kind of similar problem so it's looking like there’s something there. I think we should commit to this and improve it. Can you start exploring it?"

"Yeah definitely."
Alex was bursting with validation. They walked on out, moving on to other topics.

---

A few weeks later, Dan had managed to get some momentum behind the project. A bishop-level research data scientist was assigned to the project and more people were soon to join. As soon as Alex found out, he knew he had to bring her up-to-speed with his vision for the project, so he set up a one-on-one meeting with the data scientist as soon as he could.

Alex brought his introductory slides for the project, and went to the glass box meeting room. Alex met the researcher inside. They exchanged a "hello" and previewed each others' purposes.

```
Purposes of Bjorn (Alex's perspective):

* past hour: prepare for new project (0.8)
* past day: move on to new work project (0.6)
* past year: establish life in this city (0.5)
* lifetime: family survive (0.7)
```

It was looking like the researcher was going to be receptive. Alex was excited to get started. He got the title slide visualized in front of them both.

```
*Ground Truth*
The path to automated trust.

Alex Z. (@alexz)
Deacon-level engineer
```

Alex started,
"This is the grand vision. Automating trust. There's a few steps to get there."
Alex flashed the first slide.
```
1. Develop models that detect lies in sensors.
2. Develop a service 'Ground Truth':
	1. Covers the life of a consenting person or organization in a network of independent, decentralized sensors.
	2. Public ledger of 'Ground Truth' sensor data with sensor lie-detection.
3. Apply 'Ground Truth' to the people and organization that operate 'Ground Truth'.
4. Prove that powerful people and organizations that consent to 'Ground Truth' are higher performing, and those that don't consent are untrustworthy.
5. All those in power will adopt 'Ground Truth', and will no longer be able to hide their abuse of power.
```

Alex let the data scientist read the steps. She finished and looked back at Alex, with a slight pained smile. Alex assumed she wasn't very skilled at smiling, so he took it as a positive sign and just kept going with the same enthusiasm. Alex flicked to the next slide and continued,

```
Privacy must be the price of power.
```

"With that, we will identify the powerful that prey on the weak. Privacy must be the price of power."

He flicked to the next slide. The researcher's face had drooped a little, now stuck in slight fear, slight frown. He continued,

"First - we detect lies in sensors. What is a lie? Most people think a lie is whatever disagrees with their belief system. But really, there is no universally-accepted answer. So we have to invent our own definition."

```
A lie: when a sensor emits a sensory signal that it did not experience, and the sensor intends for someone to believe that the sensor did experience the sensory signal.
```

The data scientist had the same pain-fear expression. Alex decided this was her learning face. Alex flicked to the next slide.

"Next - the Ground Truth. A service to produce and collect decentralized data about an idea of interest (or IOI) - individuals, organizations, teams...

```
The Ground Truth

1. All data going in or out of an IOI that can reasonably be recorded, must be recorded.
Examples: all digital and analog communications to and from accounts owned or shared by the IOI, in-person activity of the IOI.
2. Past history of an IOI must be audited and recorded.
3. The assets and automations of an IOI must be monitored and recorded.
4. All IOI records must be publicly available forever, easy to perform analytical computation on, and free to use.

**IOI: idea of interest. Person, organization, team, etc.
```

The freeze-dread researcher flicked her eyes from the slides to Alex. Alex flicked to the next slide.

"Next - the Order of Ground Truth."

```
Order of Ground Truth.

The organization that tracks whether IOIs are conforming to The Ground Truth. The organization and all members of the Order are IOIs to the Ground Truth.
```

The data scientist still hadn't moved. Alex flicked to the next slide.

"And finally, we provide The Ground Truth to those in power. The Ground Truth won't apply to most people - just those in power and The Order of Ground Truth watching them. It will be inefficient, invasive, and expensive at first, but we'll build information technologies to adapt."

```
Reasons to consent to The Ground Truth:
1. You will be trusted.
2. Systematic transparency is inevitable. You can get ahead of competition by consenting now. 
```

The end. Alex felt self-assured, this was his first time going through that deck and it came together well from his perspective. He was uncomfortable that the data scientist still hadn't moved since she sat down. But it felt so fulfilling to evangelize his ideas. He was so glad he could share them - it felt exactly *right*. Alex waited a moment for the data scientist to compose herself. Eventually, he asked,
"So, do you have any questions?"

The researcher finally moved. She nodded. Dan saw fine cracks form in her marble skin and dust particles fall from the movement.

"It’ll take more than this to automate The Truth - it seems like what you’re really talking about is automating *honesty*. Any idiot can be honest while their Truth lies to your face. So I don’t have any questions, no." She started again, ”Actually, just for fun, how do you think power would use this to corrupt itself?"

Alex almost realized he should be offended, but the question was too enticing - he had thought it through quite deeply. He said,
"In theory, power is entrenched through sustained information inequality. And in theory, that inequality would come from the imperfect coverage and time delay of the sensor data feed, as well as processing power imbalances. But who knows, it boils down to execution. I guess it's an arms race over who can best maintain information inequality without breaking the rules. Just like it's always been. At least with this system, everyone should find out about the corruption same-day instead of in the history books."

"Is that so much better?"
asked the researcher.

"Yes."
said Alex. The researcher responded with a blank stare. Silence. No more questions.

"Ok, I guess we're good then."
said Alex. The data scientist nodded. They left the meeting cube.

---

A month went by. The cross-conclave project kicked off. Alex obviously wasn’t an appropriate project lead, and two others were put at the helm: a Bishop-level engineer owned the technical direction and a Bishop of engineering was responsible for project delivery. A Cardinal of Engineering smelled an endless rebuild project, so he demanded a narrowed project scope. The broader congregation came together and agreed on improving the quality of source data for purpose models. For the upcoming cycle, they'd focus on reducing noise and inconsistencies among sensors. Alex was just happy to be involved, imagining the project would eventually go the direction he had outlined.

A few weeks into the formal project, Alex went into a one-on-one with his manager, one of the Archbishops of Engineering. The Archbishop was someone who was made of things that were recently fashionable, but hadn’t quite gotten out of fashion yet. He was an idea reseller. A used idea salesman. A leader who was really a follower, who bought all the latest and greatest ideas and frameworks made outside then resold them here with a markup. A pillar of the establishment. Alex wasn’t a big fan of the establishment.

“Hey Archbishop.”
said Dan as he sat down across the glass table.

“Please, call me Bill. There’s no hierarchy here.”
said the Archbishop.

“Aren’t you above me in the management structure?”
asked Alex.

“Yes," said the Archbishop, "but I’m most comfortable speaking in simulacrum - I hope you understand."

"I really don't."
said Alex.

Alex had helped The Ground Truth get to this point, but he was underperforming in the other duties required from his role - engineering for the other near-term projects. And Alex tended to pull people in a confusing direction or cause irrelevant tangents in meetings, to the point that other leaders had asked the Archbishop to make Alex change. They asked the Archbishop to realign the direction Alex was pointing.

“That’s ok. Sensors can lie."
said the Archbishop.

"What?" said Alex, "In the meeting a few days ago, you said 'sensors can not lie.'"

The Archbishop said,
"Yes, exactly. At first, I understood how sensors can lie. But then I saw how it's more useful to 'sensors can not lie'-ify our approach at Poise. So I've joined the 'sensors can not lie' crowd for this project. But the end of the day I completely understand how you feel, *sensors can lie*. Which is what I just said."

“I don’t understand.”
said Alex.

“That’s ok Alex. I don’t understand what you say most of the time, so it’s just how things have to be - that for most of what I say, you don’t understand me.”
said the Archbishop.

“But your work is about *me*. It’s important for me to understand.”
said Alex.

“Exactly. If you understood yourself and how to contribute to Poise, I wouldn’t need to be here. But you don’t, so I do. See, we’re made to be misaligned, and misalignment is what politics is made of. If we all thought the same thing, there’d only be one of us.”
said the Archbishop.

"Anyways, where was I going next with this? Ah, yes. I wanted to share the outputs from leadership activity this week, so you have some visibility."
The Archbishop cleared his throat for the next utterance.
"*Our world-leading work has us rowing away from the waterfall-edge of the world.*"
[make paradox phrase]

Alex thought for a moment,
“I don’t understand.”

“Yes, but how does it make you feel?”
said the Archbishop.

“Like our org is doing important work, but if we don’t work hard, something bad could happen.”
Alex replied.

“Yes, exactly. The unknown. Now is the time to work hard. Perfect, that's what we were going for.”
His manager tensed up before what he said next.
"And that leads me to - I wanted to talk to you about *'you-sized hole'*."

"What? Where?"
Asked Alex.

The Archbishop spoke carefully, like he was picking every wriggling word from a bowl with chopsticks.
"There's been some… vacancies on the Order of Solutions Engineering and they need more… help. There's a *you-sized hole* in the team. Not many Orders have *you-sized hole*'d themselves in the last few cycles, you know, so I thought it'd be… a great opportunity. And when I talked to the members of the Order they all agreed - they're under a lot of pressure to deliver and they want this whole *you-sized hole* problem to end. So... *you-sized hole*. Would you be willing?"

"Of course I'll help out with that, uh… *Me-sized hole*. I've been helping them out a bit already, and I totally see the need for more help there."

"Oh that's a relief."
The Archbishop sank backwards into his chair. He was visibly relieved. Of course Alex was going to continue helping teammates on other teams - Alex was confused about why the Archbishop was so happy. He didn't see what the big deal was. But his manager was happy instead of criticizing Alex or trying to make him understand endless layers of social abstractions, so everything was a-ok for Alex.

Alex felt like maybe the Archbishop was in a good mood to ask for something from him. He said,
"Also, I was thinking... When will I be promoted to Priest-level?"

The Archbishop thought for a moment, then responded,
"Alex, we talked about this, it hasn’t changed. You still have several behaviours that are holding you back from Priest-level. Look at your purposes - your year and life purposes are still `individual survive` because your behaviour is still Solipsistic. Your technical hard skills are well beyond your level, and your soft skills are good enough, so you just need to work on your *place-in-the-universe* skills - your ability to identify with our org and align your beliefs to suit our org’s ever-changing reality. I understand Bishop Dan is helping you with that. Let’s see you improve there first, before we think about a promotion."

"Uh-huh."
Alex grunted in response.

---

Alex became more involved in customer-facing projects. He was still involved in The Ground Truth, but he spent less and less time on it. Then customer-facing projects took all his time. What happened? He didn't have time to think about it, swimming against the current - a steady stream of deadlines, bumping into milestone after milestone. The customer-facing projects were taking all his energy and then some - calls with customers on one side of the world before workdays started, then calls with customers on the other side of the world after workdays ended. Coordinating with all the teams across the world, with planning and building in between. He never asked for this kind of work, but it was happening to him. At first, it made him feel so free, then it made him feel like he was going to die, then it made him feel dead inside, and eventually he forgot what it felt like to be alive. It’s better to die fast under such circumstances - minimize the suffering to get to that sustainable death-march pace.

Alex wanted this to change. He tried bringing it up with his boss, Archbishop Bill, over several weeks. But the Archbishop was leaving the org and told Alex to talk with his mentor. Alex felt like he was the problem, and he needed to be fixed, so he was mostly decided on leaving. One last check-in with his mentor before finalizing his decision. Alex now had that meeting with Dan for their regular one-on-one right at the end of the day before the company social.

Dan was eager for the end of the day because he was hosting it. Preserving his emotional reserves and amping up to start injecting energy into everyone around him. Just one last meeting - his regular one-on-one with Alex. Dan found a meeting room quickly this time because no one was even pretending to work with the social coming up. He sat down, started going through his slides, rehearsing some phrasing and gestures in his head...

"I've been trying to figure out what to do next."
said Alex. Dan didn't even see him come in. He looked up and shook his head, blinking.
"Uh, yeah. Alex, good to see you. How have you been?”

“Yeah good, thinking about what to do next.”
Alex said, sitting down and ten mental steps ahead of Dan, waiting for him to catch up.

“How about we start with your purpose?"
said Dan.

Dan looked at Alex's purpose,

```
Purpose formulae of Alex (Dan's perspective):

past hour: "Confirm what to do next." (0.9)
past day: "Find the next thing to pursue." (0.8)
past week: "Find the next thing to pursue." (0.6)
past month: "Chase the feeling of enlightenment." (0.5)
past year: "Chase enlightenment." (0.5)
lifetime: "Survive - family." (0.6)
```

Dan thought for a moment. Alex was clearly still struggling with the same personal issues. Dan had explained the solution in the past, but Alex hadn't grasped what his problem was. Maybe his advice might finally land. Dan tried to explain again,
"Alex, you're changing your beliefs too often. You have to stick to something long enough to get anywhere. If you don't commit to things, you won't get anywhere. Even though every individual commitment is globally suboptimal, committing to nothing is worse. Commitment is an art. You have to practice committing to things, get external feedback, and commit even better next time."

Alex turned in his chair, uncomfortable with this feedback. Dan continued,
"And I've been looking at examples of other people that had the enlightenment purpose you've been stuck with this year, then got past of it. All those people followed the same pattern. They committed themselves to someone else - a newborn child or an organization or a family member or a romantic partner. That commitment carried them on a journey to believing in something greater than `survive - individual human`. They were all let down, but that's the next lesson. What you need is to find that special someone to help you transition to the next phase in your life. Someone who matters to you more than yourself."

To Alex, this sounded like a cure to a disease he had no interest in knowing about. He felt like he hadn't been understood, like Dan was talking past him. Alex said,
"I'm not ready for that. I need to improve things for *me* first. I could see myself committing to something where I'm the leader, but not where I'm a follower. That kind of commitment - to benefit someone else - won't improve things for me. The way I see it, everyone is selfish. There's no such thing as an unselfish act."

Dan stared down Alex. Alex was spewing nonsense excuses and they both knew it. This pattern of behaviour from Alex needed to change and this meeting was killing Dan's vibe. Dan spoke slowly, giving what he said more weight,
"You know what you have to do here."

Dan meant "*let someone else in*". Alex heard "*change jobs*".

Alex felt relieved that Dan agreed with what he thought he had to do. Leaving the organisationism was an awkward topic, so he quickly changed the subject. Alex said,
"Yeah agreed. If I'm in the same situation as all those people, then I can follow the patterns they established. That reminds me of something I've been thinking about - there's a thing we all seem to believe in. It's like, *data from the past should be used to predict data from the future*."
Alex said.

"Yeah, that's how the real world works. That's what Poise is based on."
Dan said.

"Well, not exactly, but ok. A thousand years ago, you would have said, 'God wills it'. Do you understand how those are our *beliefs*?"
Alex said.

"What do you mean? You can use data from the past to predict the future. That's just how things work."
Dan said.

Alex started,
"Right. But this 'how things work' is our religion. No matter what that 'how things work' is, it's our religion. What if we found out that predicting the future too much changes it in ways we don't want? Or what if we did something other than predict the future, and it gave us all the benefits with fewer problems?"

"That's ridiculous. That wouldn't make sense."
Dan said.

"Agreed - with the beliefs we have now, none of that makes sense. But what if we found a new set of beliefs with fewer problems?"
Alex said.

"So like one with spirits and ghosts and stuff? Where we make things up instead of using data to make decisions?"
Dan laughed.

"You don't get it do you?"
Alex said.

"Is this a segue to something heavy about death or something?"
Dan asked.

"Well, I plan on living forever."
said Alex.

“Really? I'm far more scared of living than I am of dying.”
Dan saw an opening to end the conversation with a boring monologue. He started,
> "Everybody dies, but not everybody lives. Dying makes sense, it’s a well-worn path. Historical data suggests death is some sort of optimal. Zero death is suboptimal. If I don't die, then there's something wrong going on. The immortal "me" would be something different. Unrecognizable. So yeah, I could try to become that forever thing which isn't me. And however long it lasts, that thing will just be a waste of resources. Or I can die - and live by spreading "me" into everything in my mortal life. I can help you and humanity and everything else to continue surviving. Which do I pick? I don’t pick. I'm spreading "me" wherever I go, through the impact I have on everything. If that involves living for quite a bit longer, so be it. In the end, we're probably just here to generate labels for the EAI, so live the way you think life should be lived to produce labels for that lifestyle. The latest model of heaven is the culmination of all the optimal ways to be, so the question isn't 'where will I go?' It's 'what parts of me will go where?'"

Dan took a moment to catch a breath. Alex butted in, trying to reiterate what he'd said before about how everyone is selfish,
"So it's only the useful ones that survive - the ones who help everyone else. They get to live forever?"

Dan started again, hoping to end the one-on-one with it,
> "Well, that’s a good place to start. But someday you'd be sorely disappointed. Your fear of death will kill you where love would let you live. You decide how you live and your species decides when you die. You're a human idea, designed to die. It doesn't feel that way because your feelings are *designed to keep you alive*. But death is a feature, not a bug. And this feature - I like to call her ‘Our Little Sister In Death’. She's a naive child, scared and lonely, who loves us all equally and unconditionally. But we dream of rejecting the poor child’s love. Ask yourself, ‘What do I need - to be ready for her loving embrace?’ Then ask yourself again. And again. Until you would be willing, at any moment, to hug her back."

Alex looked at Dan sideways. He didn't understand any of that. He could tell at this point, Dan was just making stuff up. Alex said,
"Ok. You got anything else to talk about?"

"How about we call it a day and go to the social?"
Dan answered.

"Yeah ok."
Alex said.

---

Alex quit. That week, he gave his two-month notice. His manager kept him around to close off all his projects. Alex avoided Dan for the next few weeks to avoid any more awkward conversations. Alex had also switched his AR off of the default setting to so he could see more of what was really happening at Poise. After he turned off all the transparency settings, he mostly saw a whole lot of nothing - floors and walls and tables and chairs were in the way. But he noticed things he’d never knew existed at Poise. Occasionally he saw an office cleaner or security guard, weaving between oblivious Poise workers like ghosts. Once or twice he swore he saw rats on the floor. And there were birds nesting beside some of the windows.

On his last few days in the office, Alex was scheduled for one last one-on-one with Dan. 

"Hi Alex. I heard the news about you quitting." said Dan. Dan frowned slightly, eyes flickering between eye contact and the ground.
"Why didn't you tell me earlier? You know you could have talked to me about it."

Alex responded right away,
"I don't know. My manager just like, told me not to tell anyone, so I didn't."

"You know you still could have told me."
Dan said. Silence. Dan waited for Alex to say something. An apology? Neither of them said anything.

"Where are you going next?"
Dan asked, breaking the silence.

"I'm going to be a Priest-level engineer at Recurse."
Alex said.

"I see." said Dan, "I actually interviewed there before I joined Poise. Just so you know what you're getting into, I rejected their offer because when I asked around, I found out their leadership was bad. Excessive politics."

Alex paused to ingest. It was bad news, but well-intentioned. He said,
"Thanks for sharing, but I'm pretty happy with my decision."

Dan changed the direction of conversation to something more positive.
"I really enjoyed working with you. And you know, there's one thing you have that I value quite a lot. You're always trying to improve yourself."

Alex responded automatically,
"Thank you. Well, I've learned more from you than... pretty much anyone else in the last ten years."

Dan was hit with a shot of bliss. "That means a lot to me Alex, thank you." He was stunned for a moment. Eventually he continued,
"A focus of my career right now is improving others, so when you say that I had that kind of impact on you, it's really validating. See, this is my purpose."
Dan visualized his purpose in front of them both.
[mention dans new beliefs for this phase]

```
Purpose formulae of Dan (Alex's perspective):

past hour: "Strengthen relationship with mentee."
past day: "Prepare Q3 plan."
past week: "Prepare Q3 plan."
past month: "Set up Poise engineers for success."
past year: "Set up the next generation for success."
lifetime: "Survive - humankind."

Base beliefs of Dan (Alex’s perspective):

- Something can be “better” than something else.
- The universe will continue to exist in the future.
- Ideas are “true” or “false”, and logic can be used to figure that out.
```

Alex read through Dan's purpose. Alex reflected for a moment, then he started,
"Also, on that note, I wanted to ask..."

Dan realized that Alex was going to ask to keep up the mentorship. Dan had too much going on right now, no time for that kind of commitment. He was going to have to reject Alex. Small adjustments now were about to have a big impact. Dan's eye pupils constricted to pinholes. Brain overclocking to sub-second response time in exchange for a lower PR-curve and increased rate of fatigue.

Alex was still verbally constructing the thought,
"...about mentorship"

"Look- Alex-"
Dan stuttered, trying to interrupt, shaking his head and reaching out with his hand to grab the head of the snaking sentence slithering out of this conversation’s mouth. Rejecting Alex's request for mentorship would damage their relationship. Dan's purpose formula read, `save relationship` and he didn't have to check to know it. He had to manage expectations fast.

Alex resumed, "You know me so well, so-"

"Alex- I ca-"
Dan clawed for the talking stick, but it snaked out of grasp.

Alex spoke over him, almost yelling,
"What should I look for in a new mentor?"

They maintained a glaring eye contact in silence for three seconds.

"You know, at my new place."
Alex finished, almost whispering.

Dan's heart rate was high from the fight-or-flight response. He relaxed, now recovering in a nice, deep *think*. His face lost tension, he leaned back into his chair, and let out a breath.

> "Ahhhh. Well a mentor turns your unknown unknowns into known unknowns. And a useful mentor converts useful unknowns, probably someone who's a few steps further down the dimensions you want to explore. They help you to see nearby dimensions you didn't even know exist."

Alex thought about that for a while, building up mental structures to store for the distant future. They continued talking some more. And while they talked, Alex played with his AR settings. He turned walls on and off. He turned people off, then gave them animal skins - Dan became a talking zebra in front of him, then he gave them skins that overlaid visual interpretations of their purpose - most Poise staff looked similar. He realized that Poise was just a hundred or so people walking around an ugly building, talking into AR eyesets. And they created some information that pushed out decentralized *stuff* to happen around the world.
[why is it notable that it's just people in a building? because it's easy to replicate? same with everything... Is there some message about transparency that I'm concluding here?]
[deposit what is it like in there? is it a virtual or real-life place?]

Alex finished his day at work, and went directly to a DAC Deposit (Decentralized Autonomous Organizationism). Alex submitted the pitch deck for The Order of Ground Truth. Poise was not in a position to integrate The Ground Truth into its service offerings, and Alex's new company wasn't either. Alex wanted to move on from the idea and get the invasive thoughts out of his head. The DAC Deposit is where most organizationisms come from, including Poise. The DAC Deposit had all the data of how every kind of company is made and operated - from pre-seed to liquidation - every business plan, GTM strategy, and product development process. Every legal document, sales pitch, and line of code. Every decision of every employee that has ever produced data. The DAC Deposit evaluates and starts organizationisms - funds the org, hires the staff to operate the org, and governs the org until the org is self-sufficient.

The DAC Deposit consumed Alex's pitch deck. It found "The Ground Truth" was reducible to "The One Truth". It deemed "The One Truth" to be a suboptimal model of reality, but the side-effects of an organizationism in pursuit of "The One Truth" might prove interesting. The DAC Deposit decided to spawn a DAC. It quickly calculated most hyperparameters of the new organizationism - funding rounds, customer segmentation, product roadmap... One of the hyperparameters introduced excessive variance, so the DAC Deposit initiated a grid search. It spun away, computing the optimal founding leader for "The One Truth"...

---

## Notes

Modern man is made of measuring all things.

Compensated / rewarded by society based on the value of your heuristic function.
the ones who determine the expected value would have disproportionate power, so that determination would ideally be decentralized
the determination becomes "the government" (what do you need leaders for - it's all been automated)
dude has such low value that he's a pariah

when people seem like they're cheating the system, lots of papers get quickly produced to figure out if they actually are

talk about "near-term" human milestones: colonize solar system, sustainable earth, etc.

---

Absurd constant: constant that turns theory of evolution into law.

Proportion of beliefs that are wise vs. naive in a population for it to sustain living vs. rate of new data introduced to the population.

It was important for early artificial intelligence. Someone asserted that there must be a constant to explain intelligent life, but not necessarily consciousness. In pursuit of artificial  intelligence, the world found that constant and 

---

Technology is the industry that promises next-gen tech to customers using prev-gen tech, and delivers current-gen tech. It's super-effective. Modern management made business processses consistent-enough, made software possible. Software digitised processed and data became half-collected digitally. AI adoption made trustless systems possible - multiple independent sources of "the same" observations, cheap and well-processed. Software hype, software winter, software renaissance, software commoditization. AI hype, AI winter, AI renaissance, AI commoditization. Trustless hype, trustless winter...

---

Compare yourself to someone else. You've done better than them. You've made better decisions into better life circumstances. But what if they didn't get to decisde - they made every optimal decision, just like you did. You didn't really have any control. Neither of you is better - you're both the same. Both belief systems are true. Fluidly alternate between them.

---

Anthromophise - we can’t kill an idea until it comes to life.

---

[Building made all of transparent glass and special materials and projections. People being made transparent too. If everything is transparent, you can't really see anything. There's nothing left to see.]
[the only reason you decorate things is because the alternative is something else. if the alternative was nothing, you wouldn't need to decorate]
[can see where everyone is all the time. have to just trust that there's a floor beneath you and that you won't run into walls. walls show up just before you run into them, and you get a feel for how to navigate with no visual signals]


> Humans have evolved in-built mechanisms to evangelize novel dimensions of information.