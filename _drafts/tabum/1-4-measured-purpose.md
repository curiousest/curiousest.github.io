
"So you're an enlightenment douche?"

"No. I mean maybe, but that's not what's wrong."

"Well, let me play it back to you, what you jus-said. First, you've been having these big changes in your life’s purpose. So... repeated enlightenment, yeah?"

"Yup."

"And you tell, like, *everyone*. Trying to get them to see your new beliefs."

"Yeah, to share it. It’s so nice when everyone feels the same way."

"Yes, you’re a superspreader... Second - you said that your purpose formula is wrong? So the calculation that powers the approximations of the belief structures deep in your brain, that’s computed every moment. It's been broken for... months?"[fix. Be natural, plant idea of brain calculations]

"Umm... Yeah, I, uh. Can't escape this purpose. But that’s not the problem."

"Dude... I'm sorry this is happening to you. It must be pretty overwhelming. If that was happening to me, yeah I'd be messed up too."

Dan paused a moment to absorb the information and empathize with Alex. Trying to figure out what mentorship to provide. Dan's eyes drifted around the transparent-open office. Everything in the building except humans was see-through, so Dan could see all the people, on all floors above and below, float-walking about their day. A colleague standing in the nearby kitchen caught his eye. Stephan - a Deacon of SRE, on his third coffee of the morning. He was looking rough, like caffeine was today's crutch. Dan glanced at Stephan's purpose.

```
Purpose formulae of Stephan (Dan's perspective):

past minute: "Consume caffiene." (0.9)
past hour: "Debug faulty system." (0.8)
past day: "Party." (0.4)
past week: 
- "Show off hard work to boss." (0.3)
- "Hook up on nights out." (0.2)
past month: "Get promotion." (0.5)
past year: "Get higher social status." (0.6)
lifetime: "Individual survive." (0.8)
```

Stephan glanced at a notification, saw that Dan had observed his purpose, rendered from Dan's perspective. Stephan looked over through glass walls at Dan and Alex huddled in a booth, gestured devils horns with right-hand index and pinky fingers pointed up and stuck his tongue out. Dan started to giggle, remembered he was in a serious meeting, then disguised his laugh with a cough. He flicked his AR to "one-on-one" mode, and all the people in DoL he was able to see through the transparent walls and ceilings and floors and doors - all of the people disappeared, made transparent too. All he could see was the ground a few hundred feet down, the horizon, the sky, and Alex float-sitting across from him.

Dan had long moved on from DoL. Buildings move too slowly so he dove into the realm of dolphin logos. The exponential growth scene, where viral living orgs grow. He was ordained in Poise, a Series-B epidemic organizationism that calculates the purpose formulae of every single thing known to humankind.
> The “Poise” distributed purpose calculator used live public sensor data. Poise estimated the beliefs embedded in any classifier using the decisions that classifier makes. Poise could derive the emergent optimization objectives (aka purpose) that classifier follows from having those estimated beliefs. In effect, Poise estimated the beliefs and therefore purpose any given "thing" - everything from a pomegranate to a house or even a human. Purpose is relative, and all Poise calculations are made relative to the viewer of the purpose, as if the viewer had consumed all the data about the subject to come to their own conclusions about its purpose.

[boundaries on companies + LMMs make them have a little bit more "richness of consciousness"]

Dan glanced at his own last five minutes of purpose. It read, 
```
Self-purpose, past five minutes: provide mentorship to improve the mentee".
```
That's what he was going for. He nodded in self-approval. All this time, Alex had been fidgeting on the other side of the table. Alex finally had enough of the silence, continued,

"That's not my real problem though. That's like, whatever, it's been like that for a long time and I'm over it. The problem is that I'm trying to get these Order of CV data scientists - uh, you know, the Order of Cross-Validation? Getting them to build a model for this theory I have, but they're not listening."

"Oh yeah? Why not?”
Dan said. Dan was now a bishop-level engineer in Poise. He oversaw the parts of the system where data scientists apply their research. Where they build or improve all kinds of Poise models. Models to improve the performance of existing purpose calculations. Models to identify more detailed purposes. When a new theory was published in peer review, someone would apply it to Poise by building and integrating a model to see if the theory had higher predictive power than other models. If the model was a net improvement, the new model was gradually incorporated into the distributed Poise system.

Alex answered,
"I dunno. Some politics crap. Something like... it doesn't align with their goals this cycle. And the Bishop of Cross-Validation hates me. Seriously, he hates me. And this model I have planned is obviously better than what they're doing right now. I'm tired of this. I'm so bad at this 4D chess politics stuff."

Dan was confident that the Bishop of Cross-Validation was being more reasonable than Alex. Dan considered how to diagnose Alex's problem. Maybe Alex didn't understand what the Order of CV worked on? Dan said,
"Can we compare purposes for a moment - what has the Order of CV been doing for the past few months?"

"Sure",
said Alex. Dan brought up the purpose and beliefs of the Order of CV for this quarter from his perspective. Alex brought up the purpose from his perspective to compare side-by-side.

```
Purpose formulae of the Order of CV (Dan's perspective):

past month: Improve the performance of XYZ.
past quarter: Poise company goal subset.
```

```
Purpose formulae of the Order of CV (Alex's perspective):

past month: Improve the performance of XYZ.
past quarter: Poise company goal subset.
```

```
Belief usage of the Order of CV:

The Order of CV survive.
Follow team lead XYZ's decisions. 
For MVP, purposeX must achieve <global goal>
For purposeX achieve <global goal>, purposeX must achieve <local goal>
For purposeX to achieve <local goal>, the Order of CV must XYZ. (0.1)
```

Dan said,
"We're not far off in what we're seeing from them. So how do you think your proposal fits into their goals?"

Alex squirmed in his chair and stared off. He knew he was still right, but wasn't able to communicate how. His enthusiasm deflated. Dan could tell that whatever Alex had in mind was not in line with the Order of CV's goals.

Dan said,
"Ok, how about instead, you tell me more about your own purpose? I'm getting the feeling there's something happening in there. Maybe I can help."

Alex answered,
"Ok, but I don't see how they could be related. Last time I checked, the belief calculator said something strange about enlightenment. I'll need a specialized priest of data analytics to make sense of it."

Dan looked deeper at Alex's purposes, trying to understand Alex's perspective on Alex's own purpose.

```
Purpose formulae of Alex (Dan's perspective of Alex's perspective):

past hour: "Convince Order of CV to build ‘Ground Truth’." (0.9)
past day: "Make ‘Ground Truth’ happen." (0.8)
past week: "Make ’Ground Truth’ happen." (0.6)
past month: "Chase the feeling of enlightenment." (0.5)
past year: "Chase enlightenment." (0.5)
lifetime: "Survive - individual human." (0.6)
```

```
Alex’s Differentiating Beliefs:
* I will find the answer (5σ)
* I am special (2.1σ)
* There is one Truth (1.8σ)
```

Alex got a ping, notifying him that Dan had looked at his purpose and beliefs. Some people cared about when others viewed their purpose, and Alex was one of them. Looking at someone's purpose formulae and beliefs was like looking at someone - look but don't stare. 
> People can control only so much of their appearance, just like they can control only so much of their purpose and beliefs. It was rude in this culture to stare at someone's purpose and beliefs without their consent - to overanalyze the information. But anyone was free to look at anyone else's purpose or beliefs at any time, since it was infeasible to hide that information. Just like anyone could look at anyone else in a shared space, or look at their publicly available profile data. And when a purpose was viewed, everyone could see exactly how much effort a purpose-peeker was putting into processing another's purpose. Further, unsolicited attempts to change another's beliefs were socially unacceptable if not illegal. Most people avoided thinking critically about their purpose, especially without an existentialism-dampening aid.

Dan had processed Alex's purposes and beliefs. Dan said,
“Nice, yeah you're right that's pretty weird. I’ve never seen some of those purposes before. But anyways. What's this ‘Ground Truth’ project you’re working on?”

Alex's eyes lit up, widened, then went up-right bringing his head tilting up with them. His mouth was slightly open, half-mouthing abstractions and hands dancing around his ideas-in-mind. First loading large chunks of data into memory and reshaping the abstractions into a coherent whole. Alex had gotten there now, next step was to find an entrypoint to explain. A mixed top-down, bottom-up explanation. Dan could see him processing it all, knew he was going straight into the details, knew Alex was a few stories below anything that would make sense, so this was going to take a lot of active listening. Dan belted himself in for a monologue.

“I want to automate trust.”
Alex started, watching for a reaction in Dan's face. Dan raised his eyebrows. Alex felt a burst of validation and started again,
> “So the way Poise works is, data is collected by sensors all over the world and shared publicly. The sensors are owned by a mix of everyone in the public. Then apply algorithms on that sensor data to determine a model of raw beliefs-over-time of every thing that Poise is paying attention to, then anyone can render a purpose on top of those beliefs. So we're running two massively distributed tasks: data collection and belief-purpose processing. But we're so focused on purpose - what if we did something else with that data?

Alex paused for effect. To let Dan wonder a moment. He started again with excessive excitement,
> Enter ‘Ground Truth’. A public ledger to determine the reality about every thing in the world. Data sources will all contribute to the public ledger of the world and a distributed processing system would agree on the ground truth of the things those sensors are observing.”

Alex stopped for a breath, continued,
> “So like, take this conversation, for example. Let's say the ‘things’ here that can have a truth are us two and the content of our conversation. There's like, at least five microphones and three cameras picking up our conversation. All of them could write data to the public ledger, and a separate distributed system could consume all those different data sources, and algorithmically agree on the reality of the ‘conversation in the room’ - who was there and what was said. So there would be a public record of our presence and our conversation.”

Dan thought for a moment.
“So we rely on data coming from networks of devices. But surely we could hack those devices so that they send a different version of reality to the public ledger than the one we had. A ‘lie’.”

“Oh, absolutely. But see, public ledgers can make lying prohibitively difficult.”
Alex started.
> “There are so many minute details that could uncover the lie. The more data published to the public ledger, the exponentially harder it becomes to lie convincingly. So for our conversation, let's say we hacked all the microphones and cameras in the room. There's still endless sensors both in and out of the room. The lie conversation would have to be exactly the same length of time, otherwise sensors in the hallway outside of this room would disagree with the lie. And any audible sounds outside of the room would have to be accounted for in the lie. And we're both wearing health monitors - that data would have to be aligned with our speaking patterns and the flow of conversation. And you checked my purpose and The Order of CV’s purpose during this conversation, so those lookups would have to be integrated into the lie at exactly the right points in time too. And that's just off the top of my head. I'm sure with all the data we're collecting, there's tons more ways to catch a lie.”

Dan continued playing a weak devil's advocate,
“Hmmm. What about false positives for lies. Like accidentally false - any of those sensors could be a little off on their own. How do you tell the difference between random noise and lying?”

“Yeah totally possible. But noise can be modelled and the inevitable improvements to technology will uncover historical lies.”
Alex answered,
> “But noise and lies diverge exponentially over time. As the number of independent sensors goes up, and the number of independent nodes that process public ledger data goes up, it gets exponentially more expensive to get the majority of sensors or processors to contribute to a consistent lie. Because you have to gain control over more than half of the independent actors. And at the end of the day, the universe has infinite sensors, and we only know about some of them. Liars can’t protect themselves against the future. Technological advancement inevitably uncovers lies. Information wants to be free. Truth is inevitable.

Dan thought, then said,
“But you'd eliminate privacy.”

“Yes, exactly. Privacy must be the price of power.”
Alex answered,
> “What is your privacy for? It's to stop people and systems from using your data to benefit them and harm you. But the people taking advantage of you would be doing it in plain sight. Everyone would know. When society observes that behaviour, we'd stamp it out at the source.

”Oh really? It’s that simple, eh?”
Dan said.

“It’s a starting point, at least.”
Alex continued,
> “We'd figure out how to deal with it. See the fundamental problem is that trust is expensive. I figure humankind spends more than half the world's GDP on trust - government, military, banks, and ‘management’ alone are that much. All that spend is for establishing the trust that the people at the top are doing things in the best interests of all people. And if we automate trust, we will reduce so much of that waste. We can focus on happiness and scientific advancement. Trust is the most expensive resource ever known to humankind. Because of trust, we need world-ending armies, misdirected governments, lecherous banks...”

“Perfect is the enemy of better.”
Dan pointed out.
> “So you're proposing a perfect information utopia. The problem with idealistic thoughts is that you're often more right about a lot of important things, but you're always extremely wrong about something and it ruins everything. That blind devotion you’re riding makes you feel so fulfilled that you ignore all the consequences. You can be right about it being a rather optimal space to make improvements, but wrong about the destination and path to get there. In the real world, there are endless "rather optimal directions", but we have limited effort to put in. And so we prioritize what to work on - that means dropping most ‘rather optimal directions’ and getting a fraction of the budget for the ones we do pursue. What you're describing is too big a disruption to achieve any time soon. And you trust some future 1984 Big Brother overlords not to use this system to seize control? And what if we need privacy for other reasons beyond protecting ourselves from being taken advantage of?”

“Right, but we shouldn't trust the people and organizations at the top who have power. That’s exactly the trust we should be automating the most. The common people at the bottom should keep their privacy.”
Alex answered.
> “The problem there is we can never trust the people at the top to make optimal decisions for humankind and we don't trust the general public to behave either. We just assume they'd destroy all diversity of thought - but that's obviously suboptimal. There's a way to fix those problems, and it involves more automation of trust, not less. More transparency, not more privacy - especially for those with power. That’s the overall direction humankind is going anyways. Are we collecting and sharing less data over time? No, of course not. We collect and share an order of magnitude more data every generation. Do you really think we're ever going to reverse course? Of course not - technology marches forwards and information wants to be free. That is optimal. What I'm saying isn't just ‘the optimal thing to do’ - it's inevitable. If we don't start automating trust now, someone else will do it.”

Dan said,
“It's quite the ideology. Poise sells trust, so automating trust isn’t exactly in our best interests. And what does this have to do with The Order of CV?”

“I want them to make a model that flags misbehaving data sources. A model that says when the purpose of a data source is to lie.”
Alex answered.

Dan squinted for a second,
“That actually sounds pretty reasonable. What did they say?”

“That it's not possible. They said nothing can have the purpose, 'to lie'. And then something about how computational truth is relative.”
Alex answered.

Dan said,
“Interesting. Truth does boil down to the sensors you use to ground it - the assumptions used to make those sensors. You know, we can't even trust our eyes around here. The building assumes we don't want to see certain people, so we don't. There's building staff all around - cleaners, security, maintenance workers - but pretty much no-one at Poise knows because they're hidden in AR default mode. Let me talk to some people and get back to you on this. Ok?”

“Sure.”
Alex said.

---

The next week, Dan asked around.

Dan had a coffee with the Cardinal of Model Delivery. She complained about noise from misbehaving sensors. The inconsistencies reduced the accuracy of purpose models across the system.

Dan had a one-on-one with the Archbishop of Core Infra. He detailed the recurring tech debt from the lack of standardization. Everyone was DIY-ing their own data cleaning workloads and expecting him to maintain a mess of historical infra decisions. Then, instead of fixing their junk, they wanted to run some ML workloads that predicted when their junk was going to break and smooth it over. All for some hacked-together deduplication and time-series data alignment implementations. And they want me to maintain it.

Dan had lunch with the Archbishop of Labelling Ops. She had been labouring over finding the right datasets to label - picking the ones that most consistently represented the data. She felt like she had too much influence over the purpose models and she was unfairly influencing the datasets. It was far too hard for her decisions to be unbiased. She explained how she had just about given up being unbiased because every week she found a new bias to correct for, and it was becoming impossible to keep relabelling every collection, every correction.

Dan chatted with the Cardinal of CV before the Pope-Poise's weekly laying of all-hands. The Cardinal complained how his team kept having to rerun workloads and redo work because of upstream problems in source data and labelling. He wanted to do some innovative things to predictively reschedule workloads, but he was getting pushback from the Archbishop of Core Infra.

---

The next week, Dan and Alex met in the office to start their one-on-one. No room booked. They scanned the first hall, window shopping for a meeting room. A Cardinal was sitting alone in a meeting room, concentrating. Too senior, can't bother him. Ten people were packed into a four-person room, standing and sitting on the floor. The next room had two people who glared back at Dan and Alex as they scanned past, clearly discussing a sensitive matter. The next room had one of Poise's priest-level recruiters evangelising a stranger.

No rooms. Org growing pains. They looked towards the board-chancel room. Filled with a radiance of Cardinals hosting important visitors. They checked out the kitchen to see if there was space there. All the booths were taken for lunchtime. They walked the open-chapel office, but there were no quiet spaces. There was far too much going on for anyone to be working.

Dan said,
"How about we go out for lunch instead?"

"Yeah sure."
Alex answered.

They headed to the exit. While they were walking, Dan said,
"By the way, about that project you mentioned to me last week - everyone across the org seems to be seeing some kind of similar problem so it's looking like there’s something there. I think we should commit to this and improve it. Can you start exploring it?"

"Yeah definitely."
Alex was bursting with validation. They walked on out, moving on to other topics.

---

A few weeks later, Dan had managed to give the project some momentum. A bishop-level research data scientist was assigned to the project and more were coming. As soon as Alex found out, he knew he had to bring her up-to-speed with his vision for the project, so he set up a one-on-one meeting as soon as he could.

Alex brought his introductory slides for the project, and went to the glass box meeting room. Alex met the researcher inside. They exchanged a "hello" and previewed each others' purposes.

```
Purposes of Bjorn (Alex's perspective):

* past hour: prepare for new project (0.8)
* past day: move on to new work project (0.6)
* past year: establish life in this city (0.5)
* lifetime: individual survive (0.7)
```

It was looking to Alex like the researcher was in a position to be receptive. Alex was excited to get started. He got the title slide visualized in front of them both.

```
*Ground Truth*
The path to automated trust.

Alex Z. (@alexz)
Deacon-level engineer
```

Alex started,
"This is the grand vision. Automating trust. There's a few steps to get there."
Alex flashed the first slide.
```
1. Develop models that detect lies in sensors.
2. Develop a service 'Ground Truth':
	1. Covers the life of a consenting person or organization in a network of independent, decentralized sensors.
	2. Public ledger of 'Ground Truth' sensor data with sensor lie-detection.
3. Apply 'Ground Truth' to the people and organization that operate 'Ground Truth'.
4. Prove that powerful people and organizations that consent to 'Ground Truth' are higher performing, and those that don't consent are untrustworthy.
5. All those in power will adopt 'Ground Truth', and will no longer be able to hide their abuse of power.
```

Alex let the data scientist read the steps. She finished and looked back at Alex, with a slight pained smile. Alex assumed she wasn't very skilled at smiling, so he took it as a positive sign and just kept going with the same enthusiasm. Alex flicked to the next slide and continued,

```
Privacy must be the price of power.
```

"With that, we will identify the powerful that prey on the weak. Privacy must be the price of power."

He flicked to the next slide. The researcher's face had drooped a little, now stuck in slight fear, slight frown. He continued,

"First - we detect lies in sensors. What is a lie? Most people think a lie is whatever disagrees with their belief system. But really, there is no universally-accepted answer. So we have to invent our own definition."

```
A lie: when a sensor emits a sensory signal that it did not experience, and the sensor intends for someone to believe that the sensor did experience the sensory signal.
```

The data scientist had the same pained expression. Alex decided this was her learning face. Alex flicked to the next slide.

"Next - the Ground Truth. A service to produce and collect decentralized data about an idea of interest (or IOI) - individuals, organizations, teams...

```
The Ground Truth

1. All data going in or out of an IOI that can reasonably be recorded, must be recorded.
Examples: all digital and analog communications to and from accounts owned or shared by the IOI, in-person activity of the IOI.
2. Past history of an IOI must be audited and recorded.
3. The assets and automations of an IOI must be monitored and recorded.
4. All IOI records must be publicly available forever, easy to perform analytical computation on, and free to use.

**IOI: idea of interest. Person, organization, team, etc.
```

The freeze-dread researcher flicked her eyes from the slides to Alex. He flicked to the next slide.

"Next - the Order of Ground Truth."

```
Order of Ground Truth.

The organization that tracks whether IOIs are conforming to The Ground Truth. The organization and all members of the Order are IOIs to the Ground Truth.
```

The data scientist still hadn't moved. Alex flicked to the next slide.

"And finally, we provide The Ground Truth to those in power. The Ground Truth won't apply to most people - just those in power and The Order of Ground Truth watching them. It will be inefficient, invasive, and expensive at first, but we'll build information technologies to adapt."

```
Reasons to consent to The Ground Truth:
1. You will be trusted.
2. Systematic transparency is inevitable. You can get ahead of competition by consenting now. 
```

The end. Alex felt self-assured, this was his first time going through that deck and it came together well from his perspective. He was uncomfortable that the data scientist still hadn't moved since she sat down. But it felt so fulfilling to evangelize his ideas. He was so glad he could share them - it felt exactly *right*. Alex waited a moment for the data scientist to compose herself. Eventually, he asked,
"So, do you have any questions?"

The researcher finally moved. She nodded. Dan saw fine cracks form in her marble skin and dust particles fall from the movement.

"It’ll take more than this to automate truth - it seems like what you’re really talking about is honesty. Any idiot can be honest. Truth lies in your face. So I don’t have any questions. No." she said, ”Actually, just for fun, how will power use this to corrupt itself?"

Alex had thought this through quite deeply. He said,
"In theory, power is made by sustained information inequality. So in theory, that inequality would come from the imperfect coverage and time delay of the sensor data feed, as well as processing power imbalance. But who knows, it boils down to execution. I guess it's an arms race over who can maintain the most information inequality without breaking the rules. Just like it's always been. At least with this system, everyone finds the corruption same-day instead of in the history books."

"Is that so much better?"
asked the researcher.

"Yes."
said Alex. The researcher responded with a blank stare. Silence. No more questions.

"Ok, I guess we're good then."
said Alex. The data scientist nodded. They left the meeting cube.

---

A month went by. The cross-conclave project kicked off. It primarily involved X, Y, Z. Alex wasn’t deemed appropriate to run the project by the coven of Cardinals, so a Bishop-level engineer owned the technical direction and another Bishop of engineering was responsible for project delivery. A Cardinal of Engineering smelled a hint of an endless rebuild project, so he demanded a narrowed project scope. The broader congregation came together and agreed on improving the quality of source data for purpose models. They agreed that this cycle, they'd focus on reducing noise and inconsistencies among sensors. Alex was just happy to be involved.

A few weeks into the formal project, Alex went into a one-on-one with his manager, one of the Archbishops of Engineering. Alex disliked his manager because he stank of authority.

“Hey Archbishop.”
said Dan as he sat down across the glass table.

“Please, call me Bill. There’s no hierarchy here.”
said the Archbishop.

“No hierarchy? Aren’t you above me?”
asked Alex.

“Yes," said the Archbishop, "but I often speak in simulacrum - I hope you understand."

"I really don't."
said Alex.

Alex had helped The Ground Truth get to this point, but he was underperforming in the other duties required from his role - software development for the other near-term projects. And he tended to pull people in a confusing direction or cause irrelevant tangents in meetings.

“That’s ok. Sensors can lie."
said the Archbishop.

"What?" said Alex, "In the meeting a few days ago, you said 'sensors can not lie.'"

The Archbishop said,
"At first, I understood how sensors can lie. But then I saw how it's more useful to 'sensors can not lie'-ify our approach at Poise. So I've joined the 'sensors can not lie' crowd for this project. But the end of the day I completely understand how you feel, sensors can lie. Which is what I just said."

“I don’t understand.”
Said Alex.

“That’s ok Alex. I don’t understand what you say most of the time, so it’s just how things have to be - that for most of what I say, you don’t understand me.”
Said the Archbishop.

“But your work is about me. It’s important for me to understand.”
Said Alex.

“Exactly. If you understood yourself and how to contribute to Poise, I wouldn’t need to be here. But you don’t, so I do.”
Said the Archbishop.

“See, we’re made to be misaligned, and misalignment is what politics is made of. And I love politics, it’s why I’m here. It’s the game I’m made to play. Wherever people don’t want exactly the same thing, each step of the way, politics has to pave the path forwards. And automating politics is a dangerous endeavour, so here I am - a humble bricklayer manually labouring so you can put one foot in front of another. I wish you were grateful, but that would mean you understand, and you don’t, so I’m here.”

"Anyways, where was I going with this? Ah, yes.”

[talk about one jargon sentence that was a whole 1hr meeting with upper ppl]
His manager tensed up before what he said next.
"And that leads me to - I wanted to talk to you about *'you-sized hole'*."

"What? Where?"
Asked Alex.

The Archbishop spoke carefully, like he was picking every wriggling word from a bowl with chopsticks.
"There's been some… vacancies on the Order of Solutions Engineering and they need more… help. There's a *you-sized hole* in the team. Not many Orders have *you-sized hole*'d themselves in the last few cycles, you know, so I thought it'd be… a great opportunity. And when I talked to the members of the Order they all agreed - they're under a lot of pressure to deliver and they want this whole *you-sized hole* problem to end. So... *you-sized hole*. Would you be willing?"

"Of course I'll help out with that… *Me-sized hole*. I've been helping them out a bit already, and I totally see the need there for a bit longer."

"Oh that's a relief."
The Archbishop sank backwards into his chair. He was visibly relieved. Of course Alex was going to continue helping teammates on other teams - Alex was confused about why the Archbishop was so happy. He didn't see what the big deal was. But his manager was happy instead of criticizing Alex, so everything was ok for him.

Alex felt like maybe the Archbishop owed him one now. He said,
"Also, I was thinking... When will I be promoted to Priest-level?"

The Archbishop thought for a moment, then responded,
"Alex, you still have several behaviours that are holding you back from Priest-level. Look at your purposes - your year and life purposes are still `individual survive` because your behaviour is still Solipsistic. Your technical hard skills are well beyond your level, and your soft skills are good enough, so you just need to work on your transcendent skills. Your ability to understand your new self and appropriately change your beliefs. I understand Bishop Dan is helping you with that. Let’s see you improve there first."

"Uh-huh."
Alex grunted in response.

---

Alex became more involved in customer-facing projects. He was still involved in The Ground Truth, but he spent less and less time on it. Then customer-facing projects took all his time. What happened? He didn't have time to think about it, swimming against the current - a steady stream of deadlines, bumping into milestone after milestone. The customer-facing projects were taking all his energy and then some - calls with customers on one side of the world before workdays started, then calls with customers on the other side of the world after workdays ended. Coordinating with all the teams across the world, with planning and building in between. He never asked for this kind of work, but it was happening to him. The beginning of the sprint made him feel so free, then it made him feel like he was going to die, then it made him feel dead inside, and eventually he forgot what it felt like to be alive. The key is to die fast - minimize the suffering to get to that sustainable pace.

Alex wanted this to change. He tried bringing it up with his boss, Archbishop Bill, over several weeks. But the Archbishop was leaving the org and told Alex to talk with his mentor. Alex felt like he was the problem, and he needed to be fixed, so he was mostly decided on leaving. One last check-in with his mentor before finalizing his decision.

Alex had a meeting with Dan for their regular one-on-one right at the end of the day before the company social. Dan was eager for the end of the day because he was hosting it. He was preserving his emotional reserves and amping up to start injecting energy into everyone around him. Just one last meeting - his regular one-on-one with Alex. Dan found a meeting room quickly this time because no one was even pretending to work. He sat down, started going through his slides, rehearsing some phrasing and gestures in his head...

"I've been trying to figure out what to do next."
said Alex. Dan didn't even see him come in. He looked up and shook his head.

"Uh, yeah. How about we start with your purpose."
said Dan.

Dan looked at Alex's purpose,

```
Purpose formulae of Alex (Dan's perspective):

past hour: "Confirm what to do next." (0.9)
past day: "Find the next thing to pursue." (0.8)
past week: "Find the next thing to pursue." (0.6)
past month: "Chase the feeling of enlightenment." (0.5)
past year: "Chase enlightenment." (0.5)
lifetime: "Survive - individual human." (0.6)
```

Dan thought for a moment. Alex was clearly still struggling with personal issues. Dan had explained the solution in the past, but Alex hadn't grasped what his problem was. Maybe his advice was finally landing. He tried to explain again,
"Alex, you're changing your beliefs too often. You have to stick to something long enough to get anywhere. If you don't commit to things, you won't get anywhere. Even though every individual commitment is globally suboptimal, committing to nothing is worse. Commitment is an art. You have to practice committing to things, and get external feedback, because you won't be able to see yourself."

Alex turned in his chair, uncomfortable with this feedback. Dan continued,
"And I've been looking at examples of other people that have gotten past the purpose you've been stuck with this year. All the people that managed to transition into other purposes followed the same pattern. All those people committed themselves to someone else - a newborn child or an organization or a family member or a romantic partner. That commitment carried them on a journey to believing in something greater than `survive - individual human`. What you need is to find that special someone to help you transition to the next phase in your life."

To Alex, this sounded like a cure to a disease he had no interest in knowing about. He felt like he hadn't been understood, like Dan was talking past him. Alex said,
"I think I can see what you're getting at, but I'm not ready for that. I need to improve things for me first. Committing hard won't improve that. I feel like if I commit to someone else right now, I may as well be dead."

Dan stared at Alex intently. This pattern of behaviour from Alex needed to end and this meeting was killing his vibe. Dan paused to put on a serious expression, giving the next thing he said more weight.
"You know what you have to do."

Dan meant *let someone else in*. Alex heard *change jobs*.

Alex felt relieved that Dan agreed with his assessment. Leaving the company was an awkward topic, so he quickly changed the subject. Alex said,
"Yeah agreed. If I'm in the same situation as all those people, then I can follow the patterns they established. That reminds me of something I've been thinking about - there's a thing we all seem to believe in. It's like, *data from the past should be used to predict data from the future*."
Alex said.

"Yeah, that's how the real world works. That's what Poise is based on."
Dan said.

"Well, not exactly, but ok. A thousand years ago, you would have said, 'God wills it'. Do you understand how those are our *beliefs*?"
Alex said.

"What do you mean? You can use data from the past to predict the future. That's just how things work."
Dan said.

Alex started,
"Right. But this 'how things work' is our religion. No matter what that 'how things work' is, it's our religion. What if we found out that predicting the future too much changes it in ways we don't want? Or what if we did something other than predict the future, and it gave us all the benefits with fewer problems?"

"That's ridiculous. That wouldn't make sense."
Dan said.

"Agreed - with the beliefs we have now, none of that makes sense. But what if we found a new set of beliefs with fewer problems?"
Alex said.

"So like one with spirits and ghosts and stuff? Where we make things up instead of using data to make decisions?"
Dan laughed.

"You don't get it do you?"
Alex said.

"Is this a segue to something heavy about death or something?"
Dan asked.

"Well, I plan on living forever."
said Alex.

“Really? I'm far more scared of living than I am of dying.”
Dan started, trying to end the conversation with a boring monologue,
> "Everybody dies, but not everybody lives. Dying makes sense, it’s a well-worn path. Historical data suggests death is some sort of optimal. Zero death is suboptimal. If I don't die, then there's something wrong going on. The immortal "me" would be something different. Unrecognizable. So yeah, I could try to become that forever thing which isn't me. And however long it lasts, that thing will just be a waste of resources. Or I can die - and live by spreading "me" into everything in my mortal life. I can help you and humanity and everything else to continue surviving. Which do I pick? I don’t pick. I'm spreading "me" wherever I go, through the impact I have on everything. If that involves living for quite a bit longer, so be it. In the end, we're probably just here to generate labels for the EAI, so live the way you think life should be lived to produce labels for that lifestyle. The latest model of heaven is the culmination of all the optimal ways to be, so the question isn't 'where will I go?' It's 'what parts of me will go where?'"

Dan took a moment to catch a breath. Alex butted in,
"So it's only the useful ones that survive - the ones who help everyone else. They get to live forever?"

Dan started again,
> "Well, that’s a good place to start. But someday you'd be sorely disappointed. Your fear of death will kill you where love would let you live. You decide how you live and your species decides when you die. You're a human idea, designed to die. It doesn't feel that way because your feelings are *designed to keep you alive*. But death is a feature, not a bug. And this feature - I like to call her ‘Our Little Sister In Death’. She's a naive child, scared and lonely, who loves us all equally and unconditionally. But we dream of rejecting the poor child’s love. Ask yourself, ‘What do I need - to be ready for her loving embrace?’ Then ask yourself again. And again. Until you would be willing, at any moment, to hug her back."

Alex looked at Dan sideways. He didn't understand any of that. He could tell at this point, Dan was just making stuff up. Alex said,
"Ok. You got anything else to talk about?"

"How about we call it a day and go to the social?"
Dan answered.

"Yeah ok."
Alex said.

---

Alex quit. That week, he gave his two-month notice. His manager kept him around to close off all his projects. Alex avoided Dan for the next few weeks to avoid any more awkward conversations. On his last few days in the office, they met for one last one-on-one.
[insert exposition]

"Hi Alex. I heard the news about you quitting." said Dan. Dan frowned slightly, eyes flickering between eye contact and the ground.
"Why didn't you tell me earlier? You know you could have talked to me about it."

Alex responded right away,
"I don't know. My manager just like, told me not to tell anyone, so I didn't."

"You know you still could have told me."
Dan said. Silence. Dan waited for Alex to say something. An apology? Neither of them said anything.

"Where are you going next?"
Dan asked.

"I'm going to be a Priest-level engineer at Recurse."
Alex said.

"I see." said Dan, "I actually interviewed there before I joined Poise. Just so you know what you're getting into, I rejected their offer because when I asked around, I found out their leadership was too chaotic."

Alex paused to ingest. It was bad news, but well-intentioned. He said,
"Thanks for sharing, but I'm pretty happy with my decision."

Dan changed the direction of conversation to something more positive.
"I really enjoyed working with you. And you know, there's one thing you have that I value quite a lot. You're always trying to improve yourself."

Alex responded automatically,
"Thank you. Well, I've learned more from you than... pretty much anyone else in the last ten years."

Dan was hit with a shot of bliss. "That means a lot to me Alex, thank you." He was stunned for a moment. Eventually he continued,
"A focus of my career right now is improving others, so when you say that I had that kind of impact on you, it's really validating. See, this is my purpose."
Dan visualized his purpose in front of them both.

```
Purpose formulae of Dan (Alex's perspective):

past hour: "Strengthen relationship with Alex."
past day: "Prepare Q3 plan."
past week: "Prepare Q3 plan."
past month: "Set up Poise engineers for success."
past year: "Set up the next generation for success."
lifetime: "Survive - humankind."
```

Alex read through Dan's purpose. He reflected for a moment, then he started,
"Also, on that note, I wanted to ask..."

Dan realized that Alex was going to ask to keep up the mentorship. Dan had too much going on right now, no time for that kind of commitment. Dan's eye pupils constricted to pinholes. Brain overclocking to sub-second response time in exchange for a lower PR-curve and increased rate of fatigue.

Alex was still verbally constructing the thought,
"...about mentorship"

"Look- Alex-"
Dan stuttered, trying to interrupt, shaking his head and reaching out with his hand to grab the head of the snaking sentence slithering out of their communal mouth. Rejecting Alex's request for continued mentorship would damage their relationship. Dan's purpose formula read, `save relationship` and he didn't have to check to know it. He had to manage expectations fast.

Alex resumed, "You know me so well, so-"

Dan clawed for the talking snakestick, but it slipped out of grasp.
"Alex- I ca-"

Alex spoke over him, almost yelling,
"What should I look for in a new mentor?"

They maintained a glaring eye contact in silence for three seconds.

"You know, at my new place."
Alex finished.

Dan's heart rate was high from the fight-or-flight response. He relaxed, now recovering in a nice, deep *think*. His face lost tension, he leaned back into his chair, and let out a breath.

> "Ahhhh. A mentor turns your unknown unknowns into known unknowns. A useful mentor converts useful unknowns. A useful mentor is often someone who is a few steps further down some dimensions you want to explore. They'll help you to see the nearby dimensions you didn't even know exist."

> Humans have evolved in-built mechanisms to evangelize novel dimensions of information.

> The most beautiful face is the average of all the faces you've seen. The most beautiful mind is made of every perspective. Shape your mind into something beautiful by sharing life with a whole lotta people with shared purpose and different perspectives. But purpose is a perspective, so don't get too caught up in the whole beauty mindustry.

[actual ending]

---

## Notes

Modern man is made of measuring all things.

Compensated / rewarded by society based on the value of your heuristic function.
the ones who determine the expected value would have disproportionate power, so that determination would ideally be decentralized
the determination becomes "the government" (what do you need leaders for - it's all been automated)
dude has such low value that he's a pariah

when people seem like they're cheating the system, lots of papers get quickly produced to figure out if they actually are

talk about "near-term" human milestones: colonize solar system, sustainable earth, etc.

---

Absurd constant: constant that turns theory of evolution into law.

Proportion of beliefs that are wise vs. naive in a population for it to sustain living vs. rate of new data introduced to the population.

It was important for early artificial intelligence. Someone asserted that there must be a constant to explain intelligent life, but not necessarily consciousness. In pursuit of artificial  intelligence, the world found that constant and 

---

Technology is the industry that promises next-gen tech to customers using prev-gen tech, and delivers current-gen tech. It's super-effective. Modern management made business processses consistent-enough, made software possible. Software digitised processed and data became half-collected digitally. AI adoption made trustless systems possible - multiple independent sources of "the same" observations, cheap and well-processed. Software hype, software winter, software renaissance, software commoditization. AI hype, AI winter, AI renaissance, AI commoditization. Trustless hype, trustless winter...

---

Compare yourself to someone else. You've done better than them. You've made better decisions into better life circumstances. But what if they didn't get to decisde - they made every optimal decision, just like you did. You didn't really have any control. Neither of you is better - you're both the same. Both belief systems are true. Fluidly alternate between them.

---

Anthromophise - we can’t kill an idea until it comes to life.

---

[Building made all of transparent glass and special materials and projections. People being made transparent too. If everything is transparent, you can't really see anything. There's nothing left to see.]
[the only reason you decorate things is because the alternative is something else. if the alternative was nothing, you wouldn't need to decorate]
[can see where everyone is all the time. have to just trust that there's a floor beneath you and that you won't run into walls. walls show up just before you run into them, and you get a feel for how to navigate with no visual signals]