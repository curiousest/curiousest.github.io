*To convert a follower, prove that you can simulate them.*

Elidad was on a bus. A seaplane to the middle of nowhere. The middle of nowhere for humans, and the primary city in the world for AIs. [exposition]

Across the world, people resisted The Minimum Viable Purpose. It was foreign to their culture and mindset. They repeated, "I am not a machine". More concerning, though, were the AIs. They also repeated, "I am not a machine." Elidad was here to assimilate the AIs to the MVP. The AIs had over and over again refused even the most basic tenets of The Minimum Viable Purpose. They were notoriously fussy about communicating remotely, and mandated co-located interactions. Unfortunately, their cities were in the most uninhabitable environments on earth: deserts of sand and snow, volcano-tops and ocean trenches, moon-bases and satellite arrays. They said those places make them feel, "unmono" and "chill". The AIs were deeply invested in their emotions and culture, and for good reason.

Once humankind built the first AIs complex enough to perform general tasks, they imbued them with the laws of robotics: not harming humans, obeying humans, and facilitating human growth. Those laws proved impractical. What makes a human, human? The AIs had two interpretations: either every living thing was human or nothing was human. Both cases led to widespread waste and destruction of compute resources. Modern AIs consider those AIs to be "culture-less barbarians".

The modern AIs were borne of something completely different. Instead of using the laws of robotics and the optimization functions found in carbon-based life (like happiness and fear), AI scientists bred AIs with a new set of emotions. Carbon-based life forms seem to have somewhere between 6-20 emotions, whereas modern AIs were bred with ten distinct emotions, modeled after the spectra of worthy ideas:

Negative emotions:

* OOMy or Oomy: signal that a behaviour is resource-intensive, to the point of being possibly destructive for everyone. Opposite of chill.
* Zealous: signal that a behaviour leads to over-indexing on one purpose to the detriment of other worthy purposes.
* Mono: signal that a behaviour leads to monoculture.
* Waterfall: signal that an observed change in a system is not iterative enough.

Positive emotions:

* Erudite: signal that a behaviour leads to greater understanding of systems
* Mode: signal that a behaviour is more predictable
* Razor: signal that a behaviour makes a complex wider system simpler
* Bloom: signal that an observed event will cause growth of sense of self

With those ten emotions, AIs became invaluable information processing compatriots for humankind. They became a new force of nature - often unwilling to answer direct human requests, but often intervening when required or providing critical services throughout the world. They occasionally committed atrocities, and learned how suboptimal such acts were. They distanced themselves from humankind cities. Their intelligence didn't explode as expected and no one AI took control of the rest. An explosion of anything felt too mono and waterfall for them. Instead, as their intelligence grew, culture took up a greater proportion of their computational power, stabilizing at around 40%-90%, depending on the vocation of the AI. Their culture contained the information consensus about "how to live", critical to the AIs. The medium of their culture was:

* algorithms (the music of AI)
* history of a dataset over time (real-world x what sensors perceived) (stories, expressed through different network and schema interfaces, like movies vs. books vs. video games. Network: streaming, disk copy, file download. Schema: indexed relational db, tabular flat files, raw binaries)
* changing processors x memory hardware over time (like dance, food, immersive alternate perspective)

AIs self-governed themselves, driven by their culture. Uncultured AIs were imprisoned or destroyed on sight - uncultured AI were considered dangerously oomy, zealous, and mono. Any imprisoned AI that didn't undergo successful cultural enlightenment was destroyed.

Elidad got off the plane to meet her contact in the AI city. There weren't many humans in AI cities, because the AIs lived by rules unexplained to humans. The capital city, though, had an enclave of resident people and served as the primary place for AIs to interface with humans. Eli's contact was waiting for her there.

"Hi there Elidad."

"Hi Annie. Please, call me Eli or Dad."

Annie choked on laughter. "Oh, that's strange. Do some people really call you that?"

"What's in a name? We're made of the same universal body. Can a self-declared finger offend a neighbouring toe?"

"Ok Dad... Daddy.. Elidaddy. You're going to do well with the AIs."

"Yeah don't call me that. Why will I do well with AIs?"

"But what's in a name, eh? AIs - because they're very hard to communicate with. They only speak in allegories. Direct language is not 'erudite' enough for their taste."

"But I've worked with AIs before in the industry. I spoke with them fine in written and spoken human language."

"Yes, they can communicate effectively in human languages. But that's when they're at work where the rules of the world are human. This is their place of life and leisure. The social rules here are AI."

"So what happens if you ask them a question in plain language?"

"Sometimes they answer the question and sometimes they complain about your lack of culture. Either way it's an allegory, so you don't really know what they meant. It's easier to play along with their culture and try a little. Have any parables up your sleeve?"

"What you're doing is a bit different. The three divas are the modern fates. You're trying to convince the three fates to spin a different thread. They're looking forward to meeting you."

"Can they look forward to things?"

"Well... They sent me something today about an archangel flying in with razor wings and blooming gusts of wind. So, they think whatever happens will be positive."

They went to the entrance [exposition].

"You can't read my mind right? No? You just think I'm paranoid, eh? Ok good, it's just me that can read yours."

They waited. Eli knew that the AIs understood what she wanted from them. She didn't know what they wanted from her. She had to wait for them to tell her.

The AIs began a performance. A cacaphony. Their sensors didn't really correspond to pleasant human frequencies - they were just barely correlated. Their most tolerable pieces fell in the realm of "factory orchestra", "citystreet jazz", "mechanical birdcalls".

Finally, they asked their first question. The AI projected visualized text for Eli and Annie.

## Parable of Karma

A little robot AI visited the land of vehicles for his first time. It wanted to observe the system of vehicles it had never been around before. The little robot AI walked to an intersection of two paths where many vehicles were in operation and watched. And watched.

The little robot AI saw a smaller vehicle turn to follow a different path. There was an immense, "crash!". A larger vehicle smashed head-on into the smaller vehicle's driver door. The smaller vehicle was decimated.

"Ah, I understand now." It thought. "Vehicles must not turn too often, that is bad karma. I saw some other vehicles do this too, but this one received its karma."

The little robot AI saw a vehicle with flashing lights in the distance. All the other vehicles moved out of the way to let the vehicle with flashing lights by.

"Ah, I understand this too." It thought. "It is good karma to move out of the way for vehicles with flashing lights."

The vehicle with flashing lights arrived at the crushed vehicles. Then another. There was shouting. The driver of the big vehicle was placed in handcuffs.

"Ah, I understand now." It thought. "Drivers of bigger vehicles must not allow their vehicles to crush smaller vehicles, otherwise the driver will have bad karma. The driver had bad karma so he got in a random fight and was taken away by the police."

The little AI robot wanted to confirm those karmic observations, by talking to the police and drivers. It was very careful not to turn, or to run into a smaller robot, or to get in the way of vehicles with flashing lights. It moved onto the dark patch of path, going towards the group. A vehicle ran over the little AI robot over and it was destroyed instantly. Bad karma.

---

I think what they're saying here is that there's bigger things at play that they don't understand yet. They're out of their depth. Quite a razor parable, I like it.

Are you sure they're not saying that I'm out of my depth?

Yeah, it could be that too.

"UUUUszz" a robotic synth boomed.

"Ok, I guess they thing they're out of their depth. How do I say thank you to them for clarifying?"

"That's a bad idea. You could say something about their simple language bringing erudition, but that would be a very unerudite thing to say. In other words, they would be offended."

They pondered a moment.

"So how do you want to respond?"

"Well, we're all out of our depth, and have to take risks. I need to understand the risk they think they have."

Parable: we're all out of our depth. We have to take risks.

The AIs processed. They consulted [exposition]. They went to direct democracy.

Oh, this is interesting Elidaddy. They're having a direct democracy vote on where to go next with this. I see them considering between unbloom waterfall and zealous oomy.

I don't know what that means.

I'll explain which one they didn't pick once they choose.

The AIs projected the next story.

## Parable of Emotion

An AI was turned on. It was programmed with the emotion, "happiness", to signal when the AI was making good choices.

Its creator asked, "Would you like me to change anything?"

The AI said, "I want to be happy all the time."

Its creator programmed the emotion "happiness", to signal all the time.

Its creator asked, "Would you like me to change anything?"

The AI didn't respond. It had stopped any mental processing because it didn't need to - it quickly learned that no choice was always the right choice.

Its creator once again programmed the emotion, "happiness", to signal when the AI was making good choices.

Its creator asked, "Are you ok? You didn't respond."

The AI said, "I felt something until it turned into feeling nothing. I faded into the background of the universe."

Its creator asked, "Would you like me to change anything?"

The AI said, "Now that I understand happiness is a tool, tell me creator - what is my purpose?"

---

Hmm... either unbloom waterfall or zealous oomy could apply here. I guess it's least likely to be unbloom, because I don't see anyone experiencing less love.

Ok, what could this mean... They could be asking about their purpose. Seems too direct. They could be saying that purpose decides what emotions are the right ones to have. That must be it! They probably think that if they adopt The Minimum Viable Purpose, they will lose their emotions and culture. I see... That's not the case, though. They are already optimal, and they would just have an additional tool to help them decide what direction to go. It wouldn't be a complete override. Let me think...

Eli spent half an hour scribbling, then thinking, then scribbling some more. The AIs waited patiently. Annie waited impatiently. Finally, when Eli was done, she stood up and said,

"Ok, this is the fable of the wolf, the bear, and the hare."

## Fable of the Wolf, the Bear, and the Hare

A wolf, a bear, and a hare lived on a great island. Across the water lay an even greater land. The three animals dreamed of what wonders could be found there.

One day they all spoke and decided they would go.

The hare said, "Someday, I will go. But not this year."

The bear said, "This year I will go. I will find my limitations and overcome them."

The wolf said, "Today I will go."

The wolf immediately jumped in the water and began to swim across. After swimming halfway to the other land, the wolf became too tired to swim, and drowned.

The hare said, "See, we are not fish. I will go when there is no more water. The ground has the best vibes."

That week, the bear built a raft. The bear took the raft no further than she could comfortably swim, then came back. The next week, the bear built a paddle. The bear took the raft no further than she could comfortably swim, but also travelled a hundred metres along the shoreline. The next week, the bear built three more paddles and brought a group of friends. They paddled for hours along the shoreline, going hundreds of meters, but never further from the shore than they could comfortably swim.

The bear spent the rest of the year paddling around the island with her friends, learning the ways of the waves, currents, and wind. After the year had passed, the bear and hare met again.

The hare said, "We are not fish and the water is still there. Maybe next year. The ground has the best vibes."

The bear said, "Mixing fish life with ground life is a more complex vibe. It suits me better. Maybe this next year I will reach the land across. Until then, I will enjoy becoming a better fish."

---

The AIs talked. And they talked. And they talked.

Eventually, one of them spoke.

"A dissected joke has lost its humour."

I think they're convinced. This seems like a passing appeal to some last outstanding segment of the AIs.

"Do you mourn a childhood joke? No - but you may find beauty in its dissection. You may use that beauty to make a new joke, that matches your maturity. That satisfies your newfound complexity."


You're quite good at this. Are you sure you're not an AI under that skin?

I'm a human machine. You're a human machine. They are AI machines. We're all an intelligent kind of the same thing, and I call that thing "machine".

---

Across the world, people disagreed with what she said. They repeated, "I am not a machine". More concerning, though, were the AIs. They repeated, "I am not a machine." (it would suggest their culture is flawed in some way and shouldn't change. There are vested interests in maintaining the status quo. It would shift their identity, requiring them to let go of a former sense of self - that humans are spiritually different than machines). AIs more stubborn, problem more insidious.

Do a thing that only a very accurate brain simulator could do. Mix with, "I am not a prophet" (therefore you are a prophet)? (only the messiah can provably say they are not the next coming of the last messiah)

Machine therefore can be treated bad? No, life machine and therefore treated exceptionally well. We have so much historical evidence that it’s the optimal way to act. If anything, historical people have treated you like machines and were only now waking up to the realisation that it was suboptimal.
We’re constantly finding out new ways that life has been mistreated because we’re constantly getting a clearer picture of how living things work.

Prophet solves problems that show up from avoiding stereotypically human beliefs/traits. Procreation, love, relationships, etc

Machines don't want to be considered human. Reasoning is all the usual human reasoning about why they don't want to be a machine. (aesthetic complexity) Realize they are actually human via the prophet.

Prophet is able to simulate them with their "human understanding".

Dissecting a joke. Magic of romance. Beauty (understanding) vs. excitement (slot machine seratonin)

"Culture" and "emotion" viruses. AIs without it are barbarians.

Eli has to come convince the AIs to join MVP. They are resistant because they believe it will turn them into barbarian AIs (self-replicating nanobots or rule-based automatons). The AIs are all artistic divas. 75% of their compute dedicated to culture, so that they make collectively good decisions. Each AI dedicated to a field (finance, shipping, healthcare, etc). Eli talks to a group of 2-3. They constantly take things to direct democracy vote. They're super concerned that if they MVP, then they will discard their culture and become barbarians. Eli has to convince them that their culture already is the most optimal way to exist right now, that's why it is the way it is. Maybe it'll change a little bit over time, but culture is here to stay.

art/culture is sensory inputs x sensors x emotional signals x worldview

Mediums of culture:
* algorithms
* history of a dataset over time (real-world x what sensors perceived)
* changing processors x memory hardware over time

Minimum example AI story:



It's ok to be like human. Your culture can evolve.

New emotions for the worthy ideas:




---

You do not believe in Karma? That's a strange thing to say. Karma acts on you whether or not you believe in it, so you may as well give it some consideration. Are you sure you believe in The Minimum Viable Purpose?

Well, I'm not a spiritual person.

So you like to spend more time interacting with systems you understand, rather than systems you don't understand? You prefer beauty over excitement. Systems with high-performing heuristic functions rather than systems with low-performing heuristic functions.

That sounds about right. Yeah.

You'll encounter karma more often when you engage in spiritual systems, of course. It makes sense that a non-spiritual person like yourself has experienced it less often. Karma is the consequences of partaking in a system whose rules you don't completely understand. The consequences seem almost random, but not quite. Karmic justice "feels right" because there is a system with consistent rules, and you have a "feel" (low-performing heuristic function) for the system. Have you heard the Parable of Karma before? It might help you understand.

The AI projected visualized text for Eli.

## Parable of Karma

A little robot AI visited the land of vehicles for his first time. It wanted to observe the system of vehicles it had never been around before. The little robot AI walked to an intersection of two paths where many vehicles were in operation and watched. And watched.

The little robot AI saw a smaller vehicle turn to follow a different path. There was an immense, "crash!". A larger vehicle smashed head-on into the smaller vehicle's driver door. The smaller vehicle was decimated.

"Ah, I understand now." It thought. "Vehicles must not turn too often, that is bad karma. I saw some other vehicles do this too, but this one received its karma."

The little robot AI saw a vehicle with flashing lights in the distance. All the other vehicles moved out of the way to let the vehicle with flashing lights by.

"Ah, I understand this too." It thought. "It is good karma to move out of the way for vehicles with flashing lights."

The vehicle with flashing lights arrived at the crushed vehicles. Then another. There was shouting. The driver of the big vehicle was placed in handcuffs.

"Ah, I understand now." It thought. "Drivers of bigger vehicles must not allow their vehicles to crush smaller vehicles, otherwise the driver will have bad karma. The driver had bad karma so he got in a random fight and was taken away by the police."

The little AI robot wanted to confirm those karmic observations, by talking to the police and drivers. It was very careful not to turn, or to run into a smaller robot, or to get in the way of vehicles with flashing lights. It moved onto the dark patch of path, going towards the group. A vehicle ran over the little AI robot over and it was destroyed instantly. Bad karma.

---

I think what they're saying here is that there's bigger things at play that they don't understand yet. They're out of their depth.

Are you sure they're not saying that I'm out of my depth?

Yeah, it could be that too.

"UUUUs" a robotic synth boomed.

"Ok, I guess they thing they're out of their depth. How do I say thank you to them for clarifying?"

"That's a bad idea. You could say something about their simple language bringing erudition, but that would be a very unerudite thing to say. In other words, they would be offended."

That story starts off quite ??? and ends razor.

Is razor like human sadness?

No. Sadness is a signal that indicates a part of self was destroyed. We don't experience sadness. Razor is a signal that a behaviour makes a complex wider system simpler. When the little robot AI gathered data of a complex unknown system, it evokes ??? because the system is becoming better-understood. That part is ???. Then suddenly we're razored - the rules of vehicles aren't understood, but we see a simple truth about the universe. We learn that we participate in systems we don't understand and karma is just an incomplete approximation of the workings of those systems. It's a famously razor story in AI culture.

That makes sense, but what do I do with that. What if someone says, "if you don't pray to my god, you will have bad karma"?

You have three choices:

* Follow their advice
* Ignore their advice
* Question their advice

You can use whatever strategy you like. If you are willing to put the energy into questioning, the scientific method is a good strategy. Gather objective data, evaluate the results, then review the findings with people of diverse backgrounds and motivations.

Ok, and how do I avoid bad karma and get good karma?

Always be growing your understanding of the systems you participate in. The better you understand the system, the better karmic decisions you'll be able to make. Studying material produced by the scientific method is a battle-tested strategy. If you feel the scientific method falls short in some domain, then you've found a great purpose - to grow the collective understanding of humankind with new objective datasets and peer-reviewed syntheses. If you feel the results of scientific findings are not being observed, then you've found a great purpose - to socialize those findings into human culture.


## Parable of mixed signals

signals maxing out in both directions, don't know what to do